---
title: "Analysis of Surface Data from Friends of Casco Bay Monitoring"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership"
date: "3/03/2021"
output:
  github_document:
    toc: true
    fig_width: 7
    fig_height: 5
---

<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center',
                      fig.width = 5, fig.height = 4,
                      collapse = TRUE, comment = "#>")
```

# Introduction
This Notebook analyzes FOCB's "Surface" data.
These data are pulled from long term monitoring locations around the Bay.

These are sites visited regularly by FOCB staff, either by boat or on land.  The 
focus is on warm season sampling (April through October), with roughly monthly
samples.  Earlier data from some land-based sites was collected by volunteers.

This reflects only a small portion of FOCB's monitoring program, but the surface
data provides consistent sampling history with the deepest historical record.

# Load Libraries
```{r load_libraries}
library(tidyverse)
library(readxl)

library(mgcv)     # For `gam()` and `gamm()` models
library(emmeans)

library(CBEPgraphics)
load_cbep_fonts()
theme_set(theme_cbep())
```

# Load Data
## Establish Folder Reference
```{r folder_refs}
sibfldnm <- 'Original_Data'
parent   <- dirname(getwd())
sibling  <- file.path(parent,sibfldnm)

dir.create(file.path(getwd(), 'figures'), showWarnings = FALSE)
```

## Primary Data
We specify column names because FOCB data has a row of names, a row of units,
then the data.  This approach is simpler than reading names from the first
row and correcting them to be R syntactic names.
```{r load_data, warning = FALSE}
fn    <- 'FOCB Surface All Current Sites With BSV Data.xlsx'
fpath <- file.path(sibling,fn)

mynames <- c('station', 'dt', 'time', 'sample_depth',
             'secchi', 'water_depth','temperature', 'salinity',
             'do', 'pctsat', 'pH', 'chl', 
             'month', 'year', 'fdom', 'bga', 
             'turbidity', 'blank', 'clouds', 'wndspd',
             'winddir'
             ) 

the_data <- read_excel(fpath, skip=2, col_names = mynames) %>%
  mutate(month = factor(month, levels = 1:12, labels = month.abb))

rm(mynames)
```

### Remove 2020 only data
```{r}
the_data <- the_data %>%
select(-c(fdom:winddir))
```

## Add Station Names
```{r}
fn    <- 'FOCB Monitoring Sites.xlsx'
fpath <- file.path(sibling,fn)
loc_data <- read_excel(fpath) %>%
  select(Station_ID, Station_Name) %>%
  rename(station = Station_ID,
         station_name = Station_Name)

the_data <- the_data %>%
  left_join(loc_data, by = 'station') %>%
  relocate(station_name, .after = station) %>%
  
  relocate(year, .after = dt) %>%
  relocate(month, .after = year)
```

Our data contains two stations that are not associated with 
locations that were included in our spatial data.  We can see that because
when we `left_join()` by `station`, no `station_name` value is carried over.
```{r}
l <- the_data %>%
  group_by(station) %>%
  summarize(missing = sum(is.na(station_name))) %>%
  filter(missing > 0) %>%
  pull(station)
l
```

If we look at those records, on is represented by only a single observation, and
the other only by data from 2020.  Neither matter for the current analysis. They
will get filtered out when we select data to describe recent conditions, and 
trends.
```{r}
the_data %>%
  filter(station %in% l)
```

## Address Secchi Censored Values
```{r}
the_data <- the_data %>%
  mutate(secchi_2 = if_else(secchi == "BSV", water_depth, as.numeric(secchi)),
         bottom_flag = secchi == "BSV") %>%
  relocate(secchi_2, .after = secchi) %>%
  relocate(bottom_flag, .after = secchi_2)
```


## Transform Secchi and Chlorophyll Data
We create a log plus one transformed version of the Chlorophyll data here, to
facilitate "parallel" construction of statistical models.  That transform 
regularizes variances and linearizes response fairly well. We also develop a 
square root transform of the Secchi data, for similar purposes, but the results
are not as effective.
```{r}
the_data <- the_data %>%
  mutate(sqrt_secchi = sqrt(secchi_2),
         log_chl = log(chl),
         log1_chl = log1p(chl)) %>%
  mutate(log_chl = if_else(is.infinite(log_chl) | is.nan(log_chl),
                           NA_real_, log_chl)) %>%
  relocate(sqrt_secchi, .after = secchi_2) %>%
  relocate(log_chl, log1_chl, .after = chl)
```

# Recent Conditions
In 2015, we presented marginal means and standard errors for sixteen different
regions of the Bay.  This time, we have fewer monitoring stations, and present
results for each monitoring location individually.

As in 2015, we organize results by mean or median temperature, as offshore sites
or sites strongly influenced by offshore waters do not get as warm in summer 
months as inshore sites.

## Create Recent Data
We filter to the last five FULL years of data, 2015 through 2019.
```{r}
recent_data <- the_data %>%
  filter(year > 2014 & year < 2020) %>%
  mutate(station = fct_reorder(station, temperature, mean, na.rm = TRUE),
         station_name = fct_reorder(station_name, temperature, mean, na.rm = TRUE))
```

## Summary Statistics
We want an exported data table for GIS, containing useful summary statistics,
especially medians and means.  We will probably map median values, as
they are less affected by the heavy-tailed distributions.  We also use a
portion of these data later to test which station median characteristics are 
correlated with median temperature.
```{r}
sum_data <- recent_data %>%
  select(-dt, -year, -time, -sample_depth, 
         -secchi, - bottom_flag) %>%
  relocate(water_depth, .after = month) %>%
  group_by(station) %>%
  summarize(across(c(secchi_2:log1_chl), list(mn =  ~ mean(.x, na.rm = TRUE),
                                         med = ~ median(.x, na.rm = TRUE))))

sibfldnm <- 'Derived_Data'
parent   <- dirname(getwd())
sibling  <- file.path(parent,sibfldnm)

fn    <- 'station_summary.csv'
fpath <- file.path(sibling,fn)

fpath = 
write_csv(sum_data, fpath, na = '')
```

## Create Nested Tibble
To run parallel analyses, we  reorganize the data into a  nested tibble.  We 
also add a list of labels and measurement units to simplify labeling of plots.
```{r}
units <- tibble(parameter = c('secchi_2', 'sqrt_secchi', 'temperature', 
                              'salinity', 'do',
                              'pctsat', 'pH', 
                              'chl', 'log_chl', 'log1_chl'),
                label = c("Secchi Depth", "Sqt Secchi", "Temperature",
                         "Salinity", "Dissolved Oxygen",
                         "Percent Saturation", "pH",
                         "Chlorophyll A", "Log Chlorophyll A", "Log Chlorophyll A plus 1"),
                units = c('m', '', paste0("\U00B0", "C"),
                          'PSU', 'mg/l',
                          '', '',
                          'mg/l', '', ''))

nested_data <- recent_data %>%
  select(-dt, -time, -sample_depth, 
         -secchi, - bottom_flag) %>%
  mutate(year_f = factor(year)) %>%
  relocate(water_depth, .after = month) %>%
  pivot_longer(c(secchi_2:log1_chl), names_to = 'parameter', values_to = 'value') %>%
  filter(! is.na(value)) %>%
  group_by(parameter) %>%
  nest() %>%
  left_join(units, by = 'parameter')
```

# Linear Models
```{r}
nested_data <- nested_data %>%
  mutate(lms = map(data, function(df) lm(value ~ station, data = df)))
```

## Review Model Results
### ANOVA
```{r}
for (p in nested_data$parameter) {
  cat(p)
  cat('\n')
  print(anova(nested_data$lms[nested_data$parameter == p][[1]]))
  cat('\n\n')
}
```
As suspected, all parameters differ among sites somewhere.

### Diagnostic Plots
```{r}
for (p in nested_data$parameter) {
  plot(nested_data$lms[nested_data$parameter == p][[1]],
       sub.caption = p)
}
```

So....
*  Secchi has moderate heavy tails, with weak evidence of a scale-location
   relationship.  Square root of Secchi is not much better.
*  Temperature is left skewed.  
*  Salinity has VERY heavy tails, especially at the lower end.  
*  DO has slight skew, with a heavy upper tail. that is, it's skewed right.  
*  Percent Saturation is slightly heavy tailed, with weak evidence of
   scale-location relationships.  
*  pH is heavy tailed and higher error at lower predicted pH (sites).
*  chlorophyll has a couple of outliers with moderate leverage.  It's probably 
   heavy-tailed too.  We see the best model there is on the log plus one 
   transformed chlorophyll data.

# Options for More Complex Models
We could in principal adjust models for different sampling histories
(years and months) but there is likely little value to doing so as the
sampling histories are fairly uniform, with the exception of a few sites 
added recently.

More seriously, these are time series. The measurements are not strictly 
periodic, and the interval between successive observations is large. 
Autocorrelation in the raw data is likely substantial because of seasonal 
patterns, but observations collected three weeks or a month apart are unlikely
to be mechanistically correlated, once we take into account station, season, and 
year.

We should consider interannual and month to month (day of year?) variability, 
and whether there is any advantage to modeling those variations.

## Exploratory Graphics
### Create Long Form Data
We create long-form data to facilitate faceted graphics.
```{r}
long_data <- recent_data %>%
  select(-time, -sample_depth, 
         -secchi, - bottom_flag) %>%
  relocate(water_depth, .after = month) %>%
  pivot_longer(c(secchi_2:log1_chl), names_to = 'parameter', values_to = 'value') %>%
  filter(! is.na(value))
```

### Month to Month Plot
```{r fig.width = 8, fig.height = 8}
for (p in unique(long_data$parameter)) {
  plt <- long_data %>%
    filter(parameter == p) %>%
    ggplot(aes(x = month, y = value)) +
    geom_jitter(aes(color = year), alpha = 0.5, width = 0.2, height = 0) +
    ylab(p) +
    facet_wrap(~station) +
    theme_cbep() +
    theme(axis.text.x = element_text(angle = 90))
  print(plt)
}
```

Note the relatively limited data for Chlorophyll.

Several parameters show marked seasonal patterns. Year to year changes are
harder to discern.  A few individual sites may show seasonal or year to year
variation even where that is not important across all sites, but fitting that
variation would be pushing our luck, as we are implicitly reviewing a large
number of parameters over a large number of sites, thus triggering a
multiple comparisons problem.

We may be able to use the month by month relations to slightly reduce
standard errors in our models.  Such models probably have little value for
parameter estimation here, but they may have significant value in identifying 
long-term trends.

## Hierarchical Models
We fit a year to year random factor and seasonal (monthly) terms. This reflects 
our belief that each year sets up somewhat differently due to weather, 
especially in the spring, but that some seasonal patterns shuold be robust, and
of potential direct interest to us.
```{r}
nested_data <- nested_data %>%
  mutate(lmers = map(data, function(df) gam(value ~ station + month + 
                                              s(year_f, bs = 're'), 
                                            data = df)))
```

### Review Model Results
Interestingly, year to year variation is statistically important for all
parameters.  

#### Diagnostic Plots
```{r}
for (p in nested_data$parameter) {
  gam.check(nested_data$lmers[nested_data$parameter == p][[1]],
       sub = p)
}
```
Most of those look pretty good, with the exception of salinity, which looks 
fairly awful,  and pH, which is merely bad.

Square root of Secchi depth is marginally better than raw Secchi depth, but
the difference is small, and it complicates interpretation later.

Log 1 Chlorophyll is better than either untransformed chlorophyll or log 
of chlorophyll. 

### Refit the Chlorophyll Model
We want the transformation in the model object, so we can use the tools in
`emmeans` to extract marginal means.  We refit the chlorophyll model, add it
to the nested tibble, and delete the other two chlorophyll data rows and the 
square root transformed Secchi depth row.
```{r}
df <- nested_data %>%
  filter(parameter == 'chl') %>%
  pull(data)
df <- df[[1]]  # Extract the first item in the list....

mod <- gam(log1p(value) ~ station + month +  s(year_f, bs = 're'), data = df)
```

```{r}
nested_data$lmers[nested_data$parameter == 'chl'] <- list(mod)

nested_data <- nested_data %>%
  filter(! parameter %in% c('log_chl', 'log1_chl', 'sqrt_secchi'))
```

### ANOVAS
```{r}
for (p in nested_data$parameter) {
  cat(p)
  print(anova(nested_data$lmers[nested_data$parameter == p][[1]]))
  cat('\n\n')
}
```
Note that the conclusions are that ALL parameters vary seasonally and by
location.  Year to year variation is also important for all
parameters, which is perhaps less obvious. (Note that this model 
did NOT fit interaction terms, (although they are likely to be important
for some parameters).

None of that is likely to come as a surprise.  Much of the value here is to 
confirm our intuition.

Despite high levels of statistical significance for these models, the
models are not terribly appropriate for either salinity or pH data because of 
residuals far from normally distributed

# Associations with Temperature
We emphasize in the text and presentation that temperature acts as an important 
surrogate for the relative influence of offshore waters on local conditions in 
the Bay.  If that is the case, we should be able to see high correlation with 
Temperature for the other parameters, especially after accounting for other known
covariates.

We take three approaches to check for the importance of temperature as a 
predictor of other water quality parameters:

1.   We examine whether temperature is a significant predictor of other
parameters in linear models where we treat station and year as random factors

2.  
 
 
 We need to regenerate the long-form data, retaining temperature asa predictor
 variable, not breaking it into a separate dataframe.
 
 We create long-form data to facilitate faceted graphics.

```{r}
 nested_data_2 <- recent_data %>%
  select(-dt, -time, -sample_depth, 
         -secchi, - bottom_flag) %>%
  mutate(year_f = factor(year)) %>%
  relocate(water_depth, temperature, .after = month) %>%
  pivot_longer(c(secchi_2:log1_chl), names_to = 'parameter', values_to = 'value') %>%
  filter(! is.na(value)) %>%
  group_by(parameter) %>%
  nest() %>%
  left_join(units, by = 'parameter')
 
```

## Hierarchical Model
```{r}
nested_data_2 <- nested_data_2 %>%
  mutate(lmers_temp = map(data, function(df) gam(value ~  temperature +
                                              s(year_f, bs = 're') +
                                              s(station, bs = 're'), 
                                            data = df)))
```

```{r}
for (parm in nested_data_2$parameter) {
  row = nested_data_2 %>%
    filter(parameter == parm)
  cat('\n\n')
  cat(parm)
  print(anova(row$lmers_temp[[1]]))
  }
```

So all parameters show significant correlations with temperature, treating 
Years and Stations as random factors. 

But that's not exactly the  question we want to ask. We want to ask if stations 
are structured by temperature, not if temperature correlates with other 
parameters within (random) stations and years.

## Correlations of Station Medians with Temperature
We use Kendall's Tau as a robust / resistant alternative to Pearson Correlation 
Coefficient.  A Spearman rank correlation would also be appropriate.
```{r}
tmp <- sum_data %>%
  select(station, contains('_med')) %>%
  rename_all(~sub('_med', '', .)) %>%
  relocate(temperature, .after = station)
  
for (parm in names(tmp)[3:11]) {
  cat('\n\n')
  cat(parm, '\n')
  print(cor.test(tmp$temperature, tmp[[parm]], method = "kendall"))
}
```


*  Warmer waters have:  
   *  Lower water clarity  
   *  Lower dissolved oxygen and percent saturation  
   *  Lower pH  
   * Higher chlorophyll  


